# ЁЯУШ Ceph рж╢рзЗржЦрж╛рж░ ржкрзВрж░рзНржг рж░рзЛржбржорзНржпрж╛ржк (Step-by-Step with Practical Examples)

## ЁЯФ╣ ржкрж░рзНржм рзз: Ceph ржкрж░рж┐ржЪрж┐рждрж┐ ржУ ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░

### ЁЯза **Ceph Storage ржХрзА?**

**Ceph** рж╣рж▓ ржПржХржЯрж┐ **distributed storage system**, ржпрж╛ object, block ржПржмржВ file storage ржПржХрж╕рж╛ржерзЗ рж╕рж╛ржкрзЛрж░рзНржЯ ржХрж░рзЗред ржПржЯрж╛ **high performance**, **fault-tolerant**, ржПржмржВ **scalable**ред Ceph ржорзВрж▓ржд cloud ржПржмржВ enterprise-ржЧрзНрж░рзЗржб storage solution рж╣рж┐рж╕рзЗржмрзЗ ржмрзНржпржмрж╣рзГржд рж╣ржпрж╝ред

---

### ЁЯПЧя╕П **Ceph Storage-ржПрж░ ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░ (Architecture)**

Ceph ржорзВрж▓ржд ржирж┐ржЪрзЗрж░ component-ржЧрзБрж▓рзЛ ржирж┐ржпрж╝рзЗ ржХрж╛ржЬ ржХрж░рзЗ:

1. **Ceph Monitor (MON)**
   тЖТ Cluster-ржПрж░ health, map, authentication ржЗрждрзНржпрж╛ржжрж┐ ржЯрзНрж░рзНржпрж╛ржХ ржХрж░рзЗред

2. **Ceph Manager (MGR)**
   тЖТ ржХрзНрж▓рж╛рж╕рзНржЯрж╛рж░рзЗрж░ performance data, dashboard, ржПржмржВ ржорзЗржЯрзНрж░рж┐ржХрж╕ рж╣рзНржпрж╛ржирзНржбрзЗрж▓ ржХрж░рзЗред

3. **Ceph OSD (Object Storage Daemon)**
   тЖТ ржорзВрж▓ ржбрзЗржЯрж╛ рж░рж╛ржЦрзЗ ржПржмржВ replication, recovery, backfilling ржХрж░рзЗред

4. **Ceph MDS (Metadata Server)**
   тЖТ File system (CephFS) ржПрж░ metadata manage ржХрж░рзЗред

5. **RADOS (Reliable Autonomic Distributed Object Store)**
   тЖТ Ceph-ржПрж░ ржорзВрж▓ distributed object storage layerред

---

### ЁЯУж **Ceph Storage ржЯрж╛ржЗржкрж╕**

1. **Object Storage (RADOS Gateway - RGW)**
   тЖТ Amazon S3/Swift compatible API ржжрж┐рзЯрзЗ ржХрж╛ржЬ ржХрж░рзЗред

2. **Block Storage (RBD - RADOS Block Device)**
   тЖТ Virtual machines ржмрж╛ bare-metal server ржП use ржХрж░рж╛ рж╣рзЯ (e.g., OpenStack Cinder backend)ред

3. **File System Storage (CephFS)**
   тЖТ POSIX-compliant distributed file systemред

---

### ЁЯз▒ **Ceph Cluster рждрзИрж░рж┐ ржХрж░рж╛рж░ ржзрж╛ржкрж╕ржорзВрж╣ (Step-by-Step)**

#### ЁЯз░ ржзрж╛ржк рзз: рж╕рж┐рж╕рзНржЯрзЗржо ржкрзНрж░рж╕рзНрждрзБрждрж┐

* Ubuntu/RHEL server ржкрзНрж░рж╕рзНрждрзБржд ржХрж░рзБржи (minimum 3 nodes)
* `ntp` / `chronyd` sync ржХрж░рзБржи
* Hostname ржУ `/etc/hosts` рж╕ржарж┐ржХржнрж╛ржмрзЗ ржХржиржлрж┐ржЧрж╛рж░ ржХрж░рзБржи
* ржлрж╛рзЯрж╛рж░ржУрзЯрж╛рж▓ ржУ SELinux disable ржХрж░рзБржи

### ЁЯз░ ржзрж╛ржк рзи: Ceph ржЗржирж╕рзНржЯрж▓рзЗрж╢ржи

#### ЁЯУж Manual Installation:

```bash
# Ubuntu ржПрж░ ржЬржирзНржп
sudo apt update
sudo apt install ceph-deploy ceph-common
```
#### ЁЯУж Or use Cephadm (for production)
#### ЁЯУж Or use Ceph-Ansible (for production)

### ЁЯз░ ржзрж╛ржк рзй: Cluster Bootstrapping (ceph-deploy ржжрж┐рзЯрзЗ)

```bash
ceph-deploy new mon1
ceph-deploy install mon1 osd1 osd2 mgr1
ceph-deploy mon create-initial
ceph-deploy admin mon1 osd1 osd2
```

### ЁЯз░ ржзрж╛ржк рзк: OSD ржпрзЛржЧ ржХрж░рзБржи

```bash
ceph-deploy osd create --data /dev/sdb osd1
ceph-deploy osd create --data /dev/sdb osd2
```

### ЁЯз░ ржзрж╛ржк рзл: Cluster Status ржпрж╛ржЪрж╛ржЗ

```bash
ceph -s
```

---

### ЁЯУК **Ceph Monitoring & Management**

* **Ceph Dashboard**: Web UI
* **Prometheus + Grafana** integration
* `ceph status`, `ceph df`, `ceph osd tree` CLI commands

---

### тЪЩя╕П **Ceph Storage Pools & CRUSH Map**

* **Storage Pool**: Data logical group
* **CRUSH Map**: Intelligent data placement algorithm ржпрж╛ redundancy ржУ performance ржирж┐рж╢рзНржЪрж┐ржд ржХрж░рзЗ

---

### ЁЯФР **Ceph ржПрж░ Data Redundancy & Recovery**

* **Replication**: Same data multiple OSD-рждрзЗ ржерж╛ржХрзЗ
* **Erasure Coding**: More efficient redundancy technique
* **Self-healing**: ржПржХ ржмрж╛ ржПржХрж╛ржзрж┐ржХ ржбрзЗржЯрж╛ рж▓рж╕ рж╣рж▓рзЗржУ ржирж┐ржЬрзЗ ржерзЗржХрзЗржЗ restore ржХрж░рзЗ

---

### ЁЯОп **Ceph ржПрж░ ржмрзНржпржмрж╣рж╛рж░ ржХрзНрж╖рзЗрждрзНрж░**

| Use Case           | Explanation                |
| ------------------ | -------------------------- |
| OpenStack Backend  | Cinder, Glance, Nova       |
| Kubernetes         | Rook ржжрж┐рзЯрзЗ Ceph ржЪрж╛рж▓рж╛ржирзЛ      |
| Enterprise Backup  | Large-scale object storage |
| Video Surveillance | Large file archival        |

---

---

## тЬЕ ржкрж░рзНржм рзи: Ceph Cluster рждрзИрж░рж┐ ржХрж░рж╛ (Installation + Configuration + Initial Setup)

ржПржЦрж╛ржирзЗ ржЖржорж░рж╛ ржжрзЗржЦржмрзЛ:

* рзйржЯрж┐ VM/Server ржжрж┐рзЯрзЗ Cluster ржмрж╛ржирж╛ржирзЛ
* Ceph-deploy ржжрж┐рзЯрзЗ Ceph ржЗржирж╕рзНржЯрж▓ ржХрж░рж╛
* OSD, Monitor, MGR ржХржиржлрж┐ржЧрж╛рж░ ржХрж░рж╛

---

### ЁЯЦея╕П **Step 1: рж╕рж╛рж░рзНржнрж╛рж░ ржкрзНрж░рж╕рзНрждрзБрждрж┐ (3 Node Lab Setup)**

#### ЁЯЦея╕П ржЖржорж░рж╛ ржзрж░ржЫрж┐ ржЖржорж╛ржжрзЗрж░ ржХрж╛ржЫрзЗ рзйржЯрж┐ Ubuntu Server ржЖржЫрзЗ:

| Node Name | IP Address    | Role             |
| --------- | ------------- | ---------------- |
| mon1      | 192.168.56.10 | Monitor, Manager |
| osd1      | 192.168.56.11 | OSD Node         |
| osd2      | 192.168.56.12 | OSD Node         |

---

### ЁЯЫая╕П **Step 2: ржкрзНрж░рждрж┐ржЯрж┐ рж╕рж╛рж░рзНржнрж╛рж░рзЗ ржПржЗ Common Setup ржХрж░рзБржи**

```bash
# Hostname рж╕рзЗржЯ ржХрж░рзБржи (ржкрзНрж░рждрж┐ржЯрж┐ ржирзЛржбрзЗ ржнрж┐ржирзНржиржнрж╛ржмрзЗ)
sudo hostnamectl set-hostname mon1   # ржЕржержмрж╛ osd1, osd2

# hosts ржлрж╛ржЗрж▓ ржЖржкржбрзЗржЯ ржХрж░рзБржи
sudo nano /etc/hosts
```

```bash
192.168.56.10 mon1
192.168.56.11 osd1
192.168.56.12 osd2
```

```bash
# Update & Install basic tools
sudo apt update && sudo apt install -y ntp sshpass python3-pip
```

---

### ЁЯУе **Step 3: Ceph-deploy ржЗржирж╕рзНржЯрж▓ ржХрж░рзБржи (Only on mon1)**

```bash
sudo apt install ceph-deploy -y
```

---

### ЁЯФР **Step 4: SSH Key рждрзИрж░рж┐ ржПржмржВ password-less login enable ржХрж░рзБржи**

```bash
ssh-keygen
ssh-copy-id user@osd1
ssh-copy-id user@osd2
```

---

### ЁЯУВ **Step 5: ржХрзНрж▓рж╛рж╕рзНржЯрж╛рж░ ржЗржирж┐рж╢рж┐рзЯрж╛рж▓рж╛ржЗржЬ ржХрж░рзБржи (mon1-ржП)**

```bash
mkdir ceph-cluster
cd ceph-cluster

# ржирждрзБржи cluster рждрзИрж░рж┐ ржХрж░рзБржи
ceph-deploy new mon1
```

---

### ЁЯУж **Step 6: рж╕ржорж╕рзНржд Node-ржП Ceph ржЗржирж╕рзНржЯрж▓ ржХрж░рзБржи**

```bash
ceph-deploy install mon1 osd1 osd2
```

---

### ЁЯза **Step 7: Monitor ржПржмржВ Manager рждрзИрж░рж┐ ржХрж░рзБржи**

```bash
ceph-deploy mon create-initial
ceph-deploy mgr create mon1
```

---

### ЁЯЫбя╕П **Step 8: Admin Key ржмрж┐рждрж░ржг ржХрж░рзБржи**

```bash
ceph-deploy admin mon1 osd1 osd2
chmod +r /etc/ceph/ceph.client.admin.keyring
```

---

### ЁЯТ╛ **Step 9: OSD рждрзИрж░рж┐ ржХрж░рзБржи (ржзрж░рж┐ `/dev/sdb` device ржЖржЫрзЗ)**

```bash
ceph-deploy osd create --data /dev/sdb osd1
ceph-deploy osd create --data /dev/sdb osd2
```

---

### ЁЯФН **Step 10: ржХрзНрж▓рж╛рж╕рзНржЯрж╛рж░ рж╕рзНржЯрзЗржЯрж╛рж╕ ржЪрзЗржХ ржХрж░рзБржи**

```bash
ceph -s
```

ржЙржжрж╛рж╣рж░ржг Output:

```bash
cluster:
  id:     3a1b-4e32-91a1-1e2443e9c9d6
  health: HEALTH_OK

services:
  mon: 1 daemons, quorum mon1
  mgr: mon1(active)
  osd: 2 osds: 2 up, 2 in

data:
  pools:   1 pools, 100 pgs
  objects: 0 objects, 0B
```

---

#### ЁЯОЙ ржПржЦржи ржПржХржЯрж┐ ржЪрж▓ржорж╛ржи Ceph Cluster рждрзИрж░рж┐ рж╣рзЯрзЗржЫрзЗ!

---

# тЬЕ ржкрж░рзНржм рзй: Ceph Block Storage (RBD) тАУ Complete Practical Guide

---

# ЁЯУМ 3.1 Ceph RBD ржХрзА?

**RBD (RADOS Block Device)** рж╣рж▓рзЛ Ceph-ржПрж░ Block Storage рж╕рж┐рж╕рзНржЯрзЗржоред

ржПржЯрж┐ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯ:

- KVM / QEMU Virtual Machine disk рж╣рж┐рж╕рзЗржмрзЗ
    
- OpenStack Cinder backend рж╣рж┐рж╕рзЗржмрзЗ
    
- Kubernetes Persistent Volume рж╣рж┐рж╕рзЗржмрзЗ
    
- Database storage backend рж╣рж┐рж╕рзЗржмрзЗ
    

---

# ЁЯУМ 3.2 RBD Architecture ржмрзБржЭрзЗ ржирзЗржЗ

Flow:

Application тЖТ RBD тЖТ RADOS тЖТ OSD тЖТ Disk

Ceph Block Storage ржХрж╛ржЬ ржХрж░рзЗ:

- Pool ржПрж░ ржнрж┐рждрж░рзЗ image рждрзИрж░рж┐ ржХрж░рзЗ
    
- Image = virtual disk
    
- VM рж╕рзЗржЗ image ржХрзЗ disk рж╣рж┐рж╕рзЗржмрзЗ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ
    

---

# ЁЯУМ 3.3 Step 1: RBD Pool рждрзИрж░рж┐ ржХрж░рж╛

Ceph default pool ржмрзНржпржмрж╣рж╛рж░ ржирж╛ ржХрж░рзЗ ржЖржорж░рж╛ ржЖрж▓рж╛ржжрж╛ block pool рждрзИрж░рж┐ ржХрж░ржмрзЛред

```bash
ceph osd pool create rbd_pool 128
```

Application enable ржХрж░рзБржи:

```bash
ceph osd pool application enable rbd_pool rbd
```

Pool verify ржХрж░рзБржи:

```bash
ceph osd pool ls
```

---

# ЁЯУМ 3.4 Step 2: RBD Image рждрзИрж░рж┐ ржХрж░рж╛

ржзрж░рж┐ ржЖржорж░рж╛ 10GB virtual disk рждрзИрж░рж┐ ржХрж░ржмрзЛ:

```bash
rbd create myblock1 --size 10240 --pool rbd_pool
```

List ржжрзЗржЦрзБржи:

```bash
rbd ls rbd_pool
```

Details ржжрзЗржЦрзБржи:

```bash
rbd info rbd_pool/myblock1
```

---

# ЁЯУМ 3.5 Step 3: RBD Image Map ржХрж░рж╛ (Linux Server ржП)

Client machine ржП ceph-common install ржерж╛ржХрждрзЗ рж╣ржмрзЗред

```bash
sudo apt install ceph-common
```

Image map ржХрж░рзБржи:

```bash
sudo rbd map rbd_pool/myblock1
```

Output:

```
/dev/rbd0
```

ржПржЦржи ржПржЯрж┐ ржПржХржЯрж┐ real block device рж╣рж┐рж╕рзЗржмрзЗ ржХрж╛ржЬ ржХрж░ржЫрзЗред

---

# ЁЯУМ 3.6 Step 4: Filesystem рждрзИрж░рж┐ ржУ Mount ржХрж░рж╛

```bash
sudo mkfs.ext4 /dev/rbd0
```

Mount ржХрж░рзБржи:

```bash
sudo mkdir /mnt/rbd1
sudo mount /dev/rbd0 /mnt/rbd1
```

Test ржХрж░рзБржи:

```bash
sudo touch /mnt/rbd1/testfile
```

---

# ЁЯУМ 3.7 Step 5: Unmap ржХрж░рж╛

```bash
sudo umount /mnt/rbd1
sudo rbd unmap /dev/rbd0
```

---

# ЁЯУМ 3.8 RBD Resize ржХрж░рж╛

ржзрж░рж┐ 10GB тЖТ 20GB ржХрж░рждрзЗ ржЪрж╛ржЗ

```bash
rbd resize rbd_pool/myblock1 --size 20480
```

Filesystem resize:

```bash
resize2fs /dev/rbd0
```

---

# ЁЯУМ 3.9 Snapshot рждрзИрж░рж┐ ржХрж░рж╛

Snapshot рждрзИрж░рж┐:

```bash
rbd snap create rbd_pool/myblock1@snap1
```

List:

```bash
rbd snap ls rbd_pool/myblock1
```

Rollback:

```bash
rbd snap rollback rbd_pool/myblock1@snap1
```

Delete snapshot:

```bash
rbd snap rm rbd_pool/myblock1@snap1
```

---

# ЁЯУМ 3.10 Clone рждрзИрж░рж┐ ржХрж░рж╛

Clone ржХрж░рж╛рж░ ржЖржЧрзЗ snapshot protect ржХрж░рждрзЗ рж╣ржмрзЗ:

```bash
rbd snap protect rbd_pool/myblock1@snap1
```

Clone:

```bash
rbd clone rbd_pool/myblock1@snap1 rbd_pool/myclone1
```

---

# ЁЯУМ 3.11 KVM / QEMU рждрзЗ RBD ржмрзНржпржмрж╣рж╛рж░

Install:

```bash
sudo apt install qemu-kvm libvirt-daemon-system
```

VM create ржХрж░рж╛рж░ рж╕ржорзЯ disk source рж╣рж┐рж╕рзЗржмрзЗ:

```bash
--disk path=rbd:rbd_pool/myblock1
```

Or XML config ржП:

```xml
<disk type='network' device='disk'>
  <driver name='qemu' type='raw'/>
  <source protocol='rbd' name='rbd_pool/myblock1'>
  </source>
</disk>
```

ржПржнрж╛ржмрзЗ VM disk рж╣рж┐рж╕рзЗржмрзЗ Ceph RBD ржмрзНржпржмрж╣рж╛рж░ ржХрж░ржмрзЗред

---

# ЁЯУМ 3.12 OpenStack ржП Ceph RBD Backend

OpenStack Cinder config:

File: `/etc/cinder/cinder.conf`

```ini
[ceph]
volume_driver = cinder.volume.drivers.rbd.RBDDriver
rbd_pool = rbd_pool
rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_flatten_volume_from_snapshot = false
```

Restart services:

```bash
systemctl restart cinder-volume
```

ржПржЦржи OpenStack Volume рждрзИрж░рж┐ ржХрж░рж▓рзЗ рждрж╛ Ceph ржП ржпрж╛ржмрзЗред

---

# ЁЯУМ 3.13 Kubernetes ржП Ceph RBD ржмрзНржпржмрж╣рж╛рж░ (CSI)

Modern Kubernetes ржП CSI driver ржмрзНржпржмрж╣рж╛рж░ рж╣рзЯред

Deploy:

- ceph-csi
    
- secret
    
- storageclass
    

StorageClass example:

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-rbd
provisioner: rbd.csi.ceph.com
parameters:
  pool: rbd_pool
```

PVC рждрзИрж░рж┐ ржХрж░рзБржи:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  storageClassName: ceph-rbd
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
```

Pod attach ржХрж░рж▓рзЗ dynamic RBD create рж╣ржмрзЗред

---

# ЁЯУМ 3.14 RBD Performance Tuning

Important Parameters:

```bash
ceph config set osd osd_op_threads 8
ceph config set osd osd_max_backfills 4
```

RBD Cache enable:

```bash
rbd feature enable rbd_pool/myblock1 exclusive-lock
```

---

# ЁЯУМ 3.15 Real World Use Case Example

### Scenario:

ржПржХржЯрж┐ Production Database Server

- MySQL VM running on KVM
    
- Disk backed by Ceph RBD
    
- Snapshot taken every 6 hour
    
- Clone used for testing environment
    
- If VM crash тЖТ move VM to another host тЖТ attach same RBD тЖТ start
    

Result:

- No data loss
    
- High availability
    
- Centralized storage
    

---

# ЁЯУМ 3.16 RBD Important Commands Summary

|ржХрж╛ржЬ|ржХржорж╛ржирзНржб|
|---|---|
|Pool create|ceph osd pool create|
|Image create|rbd create|
|Map|rbd map|
|Resize|rbd resize|
|Snapshot|rbd snap create|
|Clone|rbd clone|
|Delete|rbd rm|

---

# тЬЕ ржкрж░рзНржм рзк: CephFS тАУ File System рждрзИрж░рж┐ ржУ ржмрзНржпржмрж╣рж╛рж░ (Complete Practical Guide)

---

# ЁЯУМ 4.1 CephFS ржХрзА?

**CephFS** рж╣рж▓рзЛ Ceph-ржПрж░ Distributed POSIX-compliant File Systemред

ржПржЯрж┐ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯ:

- Shared storage (NFS alternative)
    
- Kubernetes shared volume
    
- Web server shared data
    
- Application cluster storage
    
- Big data analytics
    

---

# ЁЯУМ 4.2 CephFS Architecture ржмрзБржЭрзЗ ржирзЗржЗ

CephFS ржХрж╛ржЬ ржХрж░рзЗ:

Client тЖТ MDS тЖТ RADOS тЖТ OSD

ржПржЦрж╛ржирзЗ:

- **MDS (Metadata Server)** тЖТ file metadata handle ржХрж░рзЗ
    
- **OSD** тЖТ actual data store ржХрж░рзЗ
    

Block storage ржерзЗржХрзЗ ржкрж╛рж░рзНржержХрзНржп:

|RBD|CephFS|
|---|---|
|Block level|File level|
|VM disk|Shared file storage|
|Single attach|Multiple client mount|

---

# ЁЯУМ 4.3 Step 1: MDS (Metadata Server) рждрзИрж░рж┐ ржХрж░рж╛

ржЖржкржирж╛рж░ cluster ржП ржпржжрж┐ MDS ржирж╛ ржерж╛ржХрзЗ:

```bash
ceph-deploy mds create mon1
```

Status ржжрзЗржЦрзБржи:

```bash
ceph -s
```

Output ржП ржжрзЗржЦрждрзЗ ржкрж╛ржмрзЗржи:

```
mds: 1/1 daemons up
```

---

# ЁЯУМ 4.4 Step 2: CephFS ржПрж░ ржЬржирзНржп Pool рждрзИрж░рж┐ ржХрж░рж╛

CephFS ржПрж░ ржЬржирзНржп рзиржЯрж┐ pool рж▓рж╛ржЧрзЗ:

- Metadata Pool
    
- Data Pool
    

```bash
ceph osd pool create cephfs_data 128
ceph osd pool create cephfs_metadata 64
```

Application enable ржХрж░рзБржи:

```bash
ceph osd pool application enable cephfs_data cephfs
ceph osd pool application enable cephfs_metadata cephfs
```

---

# ЁЯУМ 4.5 Step 3: CephFS рждрзИрж░рж┐ ржХрж░рж╛

```bash
ceph fs new mycephfs cephfs_metadata cephfs_data
```

Verify ржХрж░рзБржи:

```bash
ceph fs ls
```

---

# ЁЯУМ 4.6 Step 4: CephFS Mount ржХрж░рж╛ (Kernel Client Method)

Client machine ржП:

```bash
sudo apt install ceph-common
```

Mount ржХрж░рзБржи:

```bash
sudo mkdir /mnt/cephfs

sudo mount -t ceph mon1:6789:/ /mnt/cephfs \
-o name=admin,secretfile=/etc/ceph/ceph.client.admin.keyring
```

Test ржХрж░рзБржи:

```bash
sudo touch /mnt/cephfs/testfile1
sudo mkdir /mnt/cephfs/project-data
```

---

# ЁЯУМ 4.7 Persistent Mount (fstab)

```bash
sudo nano /etc/fstab
```

Add:

```
mon1:6789:/ /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/ceph.client.admin.keyring,_netdev 0 0
```

---

# ЁЯУМ 4.8 FUSE ржжрж┐рзЯрзЗ Mount ржХрж░рж╛ (Alternative Method)

```bash
sudo apt install ceph-fuse
```

```bash
sudo ceph-fuse -m mon1:6789 /mnt/cephfs
```

Kernel mount ржмрзЗрж╢рж┐ performance ржжрзЗрзЯред

---

# ЁЯУМ 4.9 Subvolume рждрзИрж░рж┐ ржХрж░рж╛ (Production Use)

Enterprise environment ржП ржкрзБрж░рзЛ root share ржирж╛ ржжрж┐рзЯрзЗ subvolume ржжрзЗрзЯрж╛ рж╣рзЯред

```bash
ceph fs subvolume create mycephfs user1vol
```

List ржХрж░рзБржи:

```bash
ceph fs subvolume ls mycephfs
```

Mount path ржЬрж╛ржирзБржи:

```bash
ceph fs subvolume getpath mycephfs user1vol
```

---

# ЁЯУМ 4.10 CephFS User рждрзИрж░рж┐ (Restricted Access)

New user create:

```bash
ceph auth get-or-create client.user1 \
mon 'allow r' \
mds 'allow rw path=/volumes/_nogroup/user1vol' \
osd 'allow rw pool=cephfs_data'
```

Key save ржХрж░рзБржи:

```bash
ceph auth get-key client.user1 > user1.key
```

Mount using user1:

```bash
sudo mount -t ceph mon1:6789:/volumes/_nogroup/user1vol /mnt/user1 \
-o name=user1,secretfile=user1.key
```

---

# ЁЯУМ 4.11 Kubernetes ржП CephFS ржмрзНржпржмрж╣рж╛рж░ (CSI Driver)

CephFS CSI ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж▓рзЗ multiple pod shared storage ржкрж╛рзЯред

StorageClass example:

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: cephfs-sc
provisioner: cephfs.csi.ceph.com
parameters:
  fsName: mycephfs
  pool: cephfs_data
```

PVC:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cephfs-pvc
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: cephfs-sc
  resources:
    requests:
      storage: 5Gi
```

Pod ржП mount ржХрж░рж▓рзЗ shared storage рж╣ржмрзЗред

---

# ЁЯУМ 4.12 Real World Use Case Example

### Scenario 1: Web Server Cluster

- 3ржЯрж┐ Nginx server
    
- Shared upload folder
    
- CephFS mounted on `/var/www/html/uploads`
    

Result:

- ржпрзЗ ржХрзЛржи server ржП upload ржХрж░рж▓рзЗ рж╕ржм server ржП visible
    
- No NFS single point of failure
    
- High scalability
    

---

### Scenario 2: AI / Big Data

- Multiple processing node
    
- Shared dataset directory
    
- Parallel processing possible
    

---

# ЁЯУМ 4.13 Performance Tuning

MDS scaling:

```bash
ceph fs set mycephfs max_mds 2
```

Cache size tune:

```bash
ceph config set mds mds_cache_memory_limit 4G
```

---

# ЁЯУМ 4.14 CephFS Snapshot

Directory snapshot:

```bash
mkdir /mnt/cephfs/mydir/.snap
mkdir /mnt/cephfs/mydir/.snap/snap1
```

List snapshot:

```bash
ls /mnt/cephfs/mydir/.snap
```

---

# ЁЯУМ 4.15 CephFS Delete ржХрж░рж╛

```bash
ceph fs rm mycephfs --yes-i-really-mean-it
```

---

# ЁЯУМ 4.16 CephFS vs RBD Comparison

|Feature|RBD|CephFS|
|---|---|---|
|Access|Single client|Multiple client|
|Type|Block|File|
|Best for|VM Disk|Shared App Storage|
|Kubernetes|RWO|RWX|

---

# ЁЯОп ржкрж░рзНржм рзк рж╢рзЗрж╖рзЗ ржЖржкржирж┐ ржЬрж╛ржирж▓рзЗржи:

тЬФ CephFS Architecture  
тЬФ Pool creation  
тЬФ Filesystem create  
тЬФ Mount (Kernel & Fuse)  
тЬФ Subvolume  
тЬФ Restricted user  
тЬФ Kubernetes integration  
тЬФ Snapshot  
тЬФ Performance tuning  
тЬФ Real production use

---

# тЬЕ ржкрж░рзНржм рзл: Object Storage (RGW) ржжрж┐рзЯрзЗ S3 Bucket рждрзИрж░рж┐ (Complete Practical Guide)

---

# ЁЯУМ 5.1 Ceph Object Storage ржХрзА?

Ceph Object Storage ржХрж╛ржЬ ржХрж░рзЗ **RADOS Gateway (RGW)** ржПрж░ ржорж╛ржзрзНржпржорзЗред

ржПржЯрж┐ Compatible:

- Amazon S3 API
    
- OpenStack Swift API
    

ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯ:

- Backup storage
    
- Image storage
    
- Log storage
    
- Application object storage
    
- Kubernetes backup (Velero)
    
- Media storage
    

---

# ЁЯУМ 5.2 Ceph RGW Architecture

Client тЖТ S3 API тЖТ RGW тЖТ RADOS тЖТ OSD

ржПржЦрж╛ржирзЗ:

- RGW = S3-compatible gateway
    
- Data = object рж╣рж┐рж╕рзЗржмрзЗ store рж╣рзЯ
    
- Bucket = logical container
    

---

# ЁЯУМ 5.3 Step 1: RGW Install ржХрж░рж╛

mon1 node ржП:

```bash
ceph-deploy rgw create mon1
```

Service check ржХрж░рзБржи:

```bash
systemctl status ceph-radosgw@rgw.mon1
```

Port check ржХрж░рзБржи (default 7480):

```bash
ss -tulnp | grep rados
```

Browser ржП test ржХрж░рзБржи:

```
http://mon1:7480
```

---

# ЁЯУМ 5.4 Step 2: Object Gateway User рждрзИрж░рж┐ ржХрж░рж╛

Admin user рждрзИрж░рж┐ ржХрж░рзБржи:

```bash
radosgw-admin user create \
--uid="adminuser" \
--display-name="Admin User"
```

Output ржП ржкрж╛ржмрзЗржи:

- access_key
    
- secret_key
    

ржПржЗ key рж╕ржВрж░ржХрзНрж╖ржг ржХрж░рзБржиред

---

# ЁЯУМ 5.5 Step 3: S3 Client Install ржХрж░рж╛ (s3cmd)

Client machine ржП:

```bash
sudo apt install s3cmd
```

Configure ржХрж░рзБржи:

```bash
s3cmd --configure
```

Input ржжрж┐ржи:

```
Access Key: <your_access_key>
Secret Key: <your_secret_key>
Default Region: us-east-1
S3 Endpoint: mon1:7480
DNS-style bucket: No
```

---

# ЁЯУМ 5.6 Step 4: Bucket рждрзИрж░рж┐ ржХрж░рж╛

```bash
s3cmd mb s3://mybucket1
```

List bucket:

```bash
s3cmd ls
```

---

# ЁЯУМ 5.7 Step 5: File Upload / Download

File upload:

```bash
s3cmd put testfile.txt s3://mybucket1
```

List objects:

```bash
s3cmd ls s3://mybucket1
```

Download:

```bash
s3cmd get s3://mybucket1/testfile.txt
```

Delete:

```bash
s3cmd del s3://mybucket1/testfile.txt
```

---

# ЁЯУМ 5.8 Step 6: Bucket Policy & Public Access

Public read permission:

```bash
s3cmd setacl s3://mybucket1 --acl-public
```

Now file access:

```
http://mon1:7480/mybucket1/testfile.txt
```

---

# ЁЯУМ 5.9 Multiple User рждрзИрж░рж┐ ржХрж░рж╛

```bash
radosgw-admin user create \
--uid="appuser1" \
--display-name="Application User"
```

Quota рж╕рзЗржЯ ржХрж░рзБржи:

```bash
radosgw-admin quota set \
--uid=appuser1 \
--quota-scope=user \
--max-size=10G \
--enabled=true
```

---

# ЁЯУМ 5.10 Real World Use Case Example

---

## Scenario 1: Application Image Storage

ржзрж░рж┐ ржПржХржЯрж┐ Web Application:

- User image upload ржХрж░рзЗ
    
- Application server image store ржХрж░рзЗ Ceph bucket ржП
    
- Backend рж╢рзБржзрзБржорж╛рждрзНрж░ URL рж╕ржВрж░ржХрзНрж╖ржг ржХрж░рзЗ
    

Benefits:

- Application server disk ржмрзНржпржмрж╣рж╛рж░ рж╣рзЯ ржирж╛
    
- Scalable storage
    
- Multiple app server share same object storage
    

---

## Scenario 2: Backup Storage

- Daily database dump
    
- Cron job ржжрж┐рзЯрзЗ upload to S3
    
- Cheap & scalable storage
    
- Lifecycle policy ржжрж┐рзЯрзЗ old backup delete
    

---

## Scenario 3: Kubernetes Velero Backup

Velero config:

```yaml
apiVersion: velero.io/v1
kind: BackupStorageLocation
spec:
  provider: aws
  objectStorage:
    bucket: mybucket1
```

Velero Ceph S3 ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ backup store ржХрж░ржмрзЗред

---

# ЁЯУМ 5.11 RGW Multi-site (Advanced Concept)

Production environment ржП:

- Multiple region
    
- Bucket replication
    
- Disaster recovery
    

Command example:

```bash
radosgw-admin realm create --rgw-realm=realm1 --default
```

---

# ЁЯУМ 5.12 Performance Tuning

RGW thread increase:

```bash
ceph config set client.rgw rgw_thread_pool_size 512
```

Bucket index sharding:

```bash
radosgw-admin bucket reshard --bucket=mybucket1 --num-shards=16
```

---

# ЁЯУМ 5.13 Object Versioning Enable ржХрж░рж╛

```bash
s3cmd setversioning s3://mybucket1 enable
```

---

# ЁЯУМ 5.14 RGW Log & Troubleshooting

Log location:

```
/var/log/ceph/ceph-client.rgw.mon1.log
```

Health check:

```bash
ceph -s
```

---

# ЁЯУМ 5.15 Ceph Object vs CephFS vs RBD Comparison

|Feature|RBD|CephFS|Object|
|---|---|---|---|
|Type|Block|File|Object|
|API|Block device|POSIX|S3|
|Use|VM Disk|Shared App|Backup, Media|
|Access|Single|Multi|HTTP|

---

# тЬЕ ржкрж░рзНржм рзм: Monitoring & Web Dashboard ржЪрж╛рж▓рзБ ржХрж░рж╛ (Complete Practical Guide)

---

# ЁЯУМ 6.1 ржХрзЗржи Ceph Monitoring ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг?

Production environment ржП Monitoring ржЕрждрзНржпржирзНржд ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржХрж╛рж░ржг:

- Disk failure detect ржХрж░рж╛
    
- OSD down рж╣рж▓рзЗ alert ржкрж╛ржУрзЯрж╛
    
- Capacity planning ржХрж░рж╛
    
- Performance bottleneck ржзрж░рждрзЗ ржкрж╛рж░рж╛
    
- Cluster health realtime ржжрзЗржЦрж╛
    

---

# ЁЯУМ 6.2 Ceph Monitoring Architecture

Monitoring Layer:

- Ceph CLI
    
- Ceph Dashboard (Web UI)
    
- Prometheus
    
- Grafana
    
- Alertmanager
    

Flow:

Ceph Cluster тЖТ Metrics тЖТ Prometheus тЖТ Grafana тЖТ Alert

---

# ЁЯУМ 6.3 Step 1: Ceph Manager Module Enable ржХрж░рж╛

Ceph Dashboard MGR module ржПрж░ ржорж╛ржзрзНржпржорзЗ ржХрж╛ржЬ ржХрж░рзЗред

Check ржХрж░рзБржи:

```bash
ceph mgr module ls
```

Enable ржХрж░рзБржи:

```bash
ceph mgr module enable dashboard
```

---

# ЁЯУМ 6.4 Step 2: SSL Certificate рждрзИрж░рж┐ ржХрж░рж╛

```bash
ceph dashboard create-self-signed-cert
```

---

# ЁЯУМ 6.5 Step 3: Admin User рждрзИрж░рж┐ ржХрж░рж╛

```bash
ceph dashboard set-login-credentials admin admin123
```

---

# ЁЯУМ 6.6 Step 4: Dashboard Port Check

Default port: 8443

Check ржХрж░рзБржи:

```bash
ceph mgr services
```

Browser ржП ржпрж╛ржи:

```
https://mon1:8443
```

Login:

- Username: admin
    
- Password: admin123
    

---

# ЁЯУМ 6.7 Dashboard ржП ржХрзА ржХрзА ржжрзЗржЦрждрзЗ ржкрж╛ржмрзЗржи?

Dashboard Overview ржП:

- Cluster health
    
- OSD status
    
- MON status
    
- MDS status
    
- Pool usage
    
- Object count
    
- Throughput graph
    
- IOPS graph
    

---

# ЁЯУМ 6.8 Real World Monitoring Scenario

### Scenario 1: OSD Disk Failure

ржпржжрж┐ ржПржХржЯрж┐ disk fail ржХрж░рзЗ:

```bash
ceph osd out osd.1
```

Dashboard ржП ржжрзЗржЦржмрзЗржи:

- HEALTH_WARN
    
- PG degraded
    
- Recovery progress bar
    

ржЖржкржирж┐ visually recovery progress ржжрзЗржЦрждрзЗ ржкрж╛рж░ржмрзЗржиред

---

### Scenario 2: Capacity Planning

Dashboard тЖТ Pools тЖТ Usage

ржзрж░рж┐:

- rbd_pool 75% full
    

ржЖржкржирж┐ ржЖржЧрзЗржЗ ржирждрзБржи OSD ржпрзЛржЧ ржХрж░рждрзЗ ржкрж╛рж░ржмрзЗржиред

---

# ЁЯУМ 6.9 Prometheus Enable ржХрж░рж╛

```bash
ceph mgr module enable prometheus
```

Check endpoint:

```
http://mon1:9283/metrics
```

---

# ЁЯУМ 6.10 Grafana Enable ржХрж░рж╛

```bash
ceph mgr module enable grafana
```

Install Grafana (if not installed):

```bash
sudo apt install grafana
```

Start service:

```bash
systemctl start grafana-server
```

Access:

```
http://mon1:3000
```

Default login:

- admin
    
- admin
    

---

# ЁЯУМ 6.11 Ceph Built-in Grafana Dashboard

Ceph automatically import dashboard templatesред

Metrics ржкрж╛ржмрзЗржи:

- OSD latency
    
- Read/Write IOPS
    
- Throughput
    
- PG state
    
- CPU usage
    
- Disk utilization
    

---

# ЁЯУМ 6.12 Alertmanager Setup

Enable:

```bash
ceph mgr module enable alertmanager
```

Alert example:

- OSD down
    
- Disk full
    
- MON quorum lost
    
- Slow requests
    

Email alert configure ржХрж░рждрзЗ рж╣рж▓рзЗ:

```bash
ceph dashboard set-alertmanager-api-host http://localhost:9093
```

---

# ЁЯУМ 6.13 Important CLI Monitoring Commands

Cluster health:

```bash
ceph -s
```

Detailed health:

```bash
ceph health detail
```

OSD tree:

```bash
ceph osd tree
```

Pool usage:

```bash
ceph df
```

IO performance:

```bash
ceph osd perf
```

PG status:

```bash
ceph pg stat
```

---

# ЁЯУМ 6.14 Log Monitoring

Log location:

```
/var/log/ceph/
```

Live log:

```bash
tail -f /var/log/ceph/ceph.log
```

---

# ЁЯУМ 6.15 Real Production Monitoring Example

## Scenario: Production Kubernetes Cluster

- RBD storage for database
    
- CephFS for shared storage
    
- RGW for backup
    

Monitoring setup:

- Dashboard for quick view
    
- Grafana for deep performance metrics
    
- Alertmanager for email alert
    
- Weekly capacity review
    

Result:

- Proactive issue detection
    
- Zero surprise outage
    
- Scalable planning
    

---

# ЁЯУМ 6.16 Performance Analysis Example

High latency detect ржХрж░рж▓рзЗ:

```bash
ceph osd perf
```

High commit latency ржорж╛ржирзЗ:

- Disk slow
    
- Network issue
    
- CPU bottleneck
    

Dashboard graph ржжрж┐рзЯрзЗ root cause identify ржХрж░рж╛ ржпрж╛рзЯред

---

# ЁЯУМ 6.17 Security Best Practice

Dashboard password change:

```bash
ceph dashboard ac-user-set-password admin
```

HTTPS mandatory рж░рж╛ржЦрзБржиред

Firewall open ржХрж░рзБржи рж╢рзБржзрзБржорж╛рждрзНрж░ trusted IP ржПрж░ ржЬржирзНржпред

---

# тЬЕ ржкрж░рзНржм рзн: Disaster Recovery, Performance Tuning & Pool Management (Complete Practical Guide)

ржПржЯрж┐ рж╕ржмржЪрзЗрзЯрзЗ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржЕржзрзНржпрж╛рзЯред  
Production environment ржП Ceph ржЪрж╛рж▓рж╛рждрзЗ рж╣рж▓рзЗ ржПржЗ ржЕржВрж╢ ржнрж╛рж▓рзЛржнрж╛ржмрзЗ ржмрзБржЭрждрзЗржЗ рж╣ржмрзЗред

---

# ЁЯУМ 7.1 Disaster Recovery (DR) Overview

Ceph naturally fault-tolerant ржХрж╛рж░ржг:

- Data replication
    
- Self-healing
    
- No single point of failure
    

ржХрж┐ржирзНрждрзБ Production DR ржорж╛ржирзЗ:

- Node failure recovery
    
- OSD failure recovery
    
- Pool recovery
    
- Cluster backup
    
- Multi-site replication
    

---

# ЁЯУМ 7.2 OSD Failure Recovery (Real Scenario)

### Scenario:

ржПржХржЯрж┐ disk рж╣ржарж╛рзО ржирж╖рзНржЯ рж╣рзЯрзЗ ржЧрзЗржЫрзЗред

Check ржХрж░рзБржи:

```bash
ceph -s
```

Output:

```
HEALTH_WARN
1 osd down
```

Identify ржХрж░рзБржи:

```bash
ceph osd tree
```

---

## Step 1: OSD mark out ржХрж░рзБржи

```bash
ceph osd out osd.1
```

Cluster rebalancing рж╢рзБрж░рзБ рж╣ржмрзЗред

---

## Step 2: Disk replace ржХрж░рзБржи

New disk detect рж╣рж▓рзЗ:

```bash
ceph-volume lvm create --data /dev/sdb
```

---

## Step 3: Verify recovery

```bash
ceph -s
```

HEALTH_OK ржирж╛ ржЖрж╕рж╛ ржкрж░рзНржпржирзНржд ржЕржкрзЗржХрзНрж╖рж╛ ржХрж░рзБржиред

---

# ЁЯУМ 7.3 Node Failure Recovery

### Scenario:

ржкрзБрж░рзЛ ржПржХржЯрж┐ OSD node downред

Cluster automatically:

- Replication maintain ржХрж░ржмрзЗ
    
- Data ржЕржирзНржп OSD рждрзЗ redistribute ржХрж░ржмрзЗ
    

Node repair рж╣рж▓рзЗ:

```bash
systemctl start ceph-osd.target
```

---

# ЁЯУМ 7.4 Pool Replication Management

Replication check ржХрж░рзБржи:

```bash
ceph osd pool get rbd_pool size
```

Replication change ржХрж░рзБржи:

```bash
ceph osd pool set rbd_pool size 3
```

Meaning:

- size=3 тЖТ 3 copies ржерж╛ржХржмрзЗ
    

Minimum replica:

```bash
ceph osd pool set rbd_pool min_size 2
```

---

# ЁЯУМ 7.5 Erasure Coding (Storage Efficiency)

Replication expensiveред

Erasure coding efficientред

Create EC profile:

```bash
ceph osd erasure-code-profile set ecprofile k=2 m=1
```

Create EC pool:

```bash
ceph osd pool create ecpool 128 128 erasure ecprofile
```

Use case:

- Backup storage
    
- Archive storage
    

---

# ЁЯУМ 7.6 CRUSH Map (Data Placement Control)

CRUSH algorithm determine ржХрж░рзЗ:

- ржХрзЛржи OSD рждрзЗ data ржпрж╛ржмрзЗ
    
- Rack awareness
    
- Failure domain
    

Check rule:

```bash
ceph osd crush rule ls
```

Custom rule create (rack based):

```bash
ceph osd crush rule create-replicated rack_rule default rack
```

Apply to pool:

```bash
ceph osd pool set rbd_pool crush_rule rack_rule
```

Production datacenter ржП ржЕрждрзНржпржирзНржд ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржгред

---

# ЁЯУМ 7.7 Performance Tuning (Advanced)

---

## 1я╕ПтГг OSD Thread Tune

```bash
ceph config set osd osd_op_threads 8
```

---

## 2я╕ПтГг Backfill Limit

```bash
ceph config set osd osd_max_backfills 4
```

---

## 3я╕ПтГг Recovery Speed Tune

```bash
ceph config set osd osd_recovery_max_active 5
```

---

## 4я╕ПтГг BlueStore Optimization

```bash
ceph config set osd bluestore_cache_size 4G
```

---

# ЁЯУМ 7.8 Network Optimization

Production recommendation:

- 10Gbps network
    
- Separate public & cluster network
    

ceph.conf example:

```
public network = 192.168.1.0/24
cluster network = 10.0.0.0/24
```

---

# ЁЯУМ 7.9 Backup Strategy (Critical Part)

Ceph backup options:

1. RBD snapshot export
    
2. RGW multi-site replication
    
3. CephFS snapshot
    
4. External backup system
    

---

## RBD Export Example

```bash
rbd export rbd_pool/myblock1 backup.img
```

Restore:

```bash
rbd import backup.img rbd_pool/restoredblock
```

---

# ЁЯУМ 7.10 Cluster Backup (Config Backup)

Backup these files:

```
/etc/ceph/
/var/lib/ceph/
```

---

# ЁЯУМ 7.11 Pool Management

List pools:

```bash
ceph osd pool ls
```

Delete pool:

```bash
ceph osd pool delete testpool testpool --yes-i-really-really-mean-it
```

Rename pool:

```bash
ceph osd pool rename oldname newname
```

---

# ЁЯУМ 7.12 PG (Placement Group) Optimization

Check PG:

```bash
ceph osd pool get rbd_pool pg_num
```

Increase PG:

```bash
ceph osd pool set rbd_pool pg_num 256
```

Rule:

PG тЙИ (OSD * 100) / replica size

---

# ЁЯУМ 7.13 Real Production Disaster Scenario

### Scenario:

Datacenter A completely downред

Solution:

- Multi-site RGW replication
    
- Secondary Ceph cluster
    
- Backup import
    
- DNS failover
    

Result:

- Minimal downtime
    
- Business continuity
    

---

# ЁЯУМ 7.14 Cluster Upgrade Strategy

Upgrade safely:

```bash
ceph orch upgrade start --ceph-version <version>
```

Upgrade order:

1. MON
    
2. MGR
    
3. OSD
    
4. MDS
    
5. RGW
    

Never upgrade all at onceред

---

# ЁЯУМ 7.15 High Availability Best Practice Summary

тЬФ Minimum 3 MON  
тЬФ Odd number MON  
тЬФ Multiple OSD per node  
тЬФ Separate network  
тЬФ SSD for DB/WAL  
тЬФ Monitoring + Alert  
тЬФ Regular snapshot  
тЬФ Periodic health audit

---

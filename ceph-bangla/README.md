# üìò Ceph ‡¶∂‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶∞‡ßã‡¶°‡¶Æ‡ßç‡¶Ø‡¶æ‡¶™ (Step-by-Step with Practical Examples)

## üîπ ‡¶™‡¶∞‡ßç‡¶¨ ‡ßß: Ceph ‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶ø ‡¶ì ‡¶Ü‡¶∞‡ßç‡¶ï‡¶ø‡¶ü‡ßá‡¶ï‡¶ö‡¶æ‡¶∞

### üß† **Ceph Storage ‡¶ï‡ßÄ?**

**Ceph** ‡¶π‡¶≤ ‡¶è‡¶ï‡¶ü‡¶ø **distributed storage system**, ‡¶Ø‡¶æ object, block ‡¶è‡¶¨‡¶Ç file storage ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶∏‡¶æ‡¶™‡ßã‡¶∞‡ßç‡¶ü ‡¶ï‡¶∞‡ßá‡•§ ‡¶è‡¶ü‡¶æ **high performance**, **fault-tolerant**, ‡¶è‡¶¨‡¶Ç **scalable**‡•§ Ceph ‡¶Æ‡ßÇ‡¶≤‡¶§ cloud ‡¶è‡¶¨‡¶Ç enterprise-‡¶ó‡ßç‡¶∞‡ßá‡¶° storage solution ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡¶Ø‡¶º‡•§

---

### üèóÔ∏è **Ceph Storage-‡¶è‡¶∞ ‡¶Ü‡¶∞‡ßç‡¶ï‡¶ø‡¶ü‡ßá‡¶ï‡¶ö‡¶æ‡¶∞ (Architecture)**

Ceph ‡¶Æ‡ßÇ‡¶≤‡¶§ ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ component-‡¶ó‡ßÅ‡¶≤‡ßã ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá:

1. **Ceph Monitor (MON)**
   ‚Üí Cluster-‡¶è‡¶∞ health, map, authentication ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø ‡¶ü‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï ‡¶ï‡¶∞‡ßá‡•§

2. **Ceph Manager (MGR)**
   ‚Üí ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡ßá‡¶∞ performance data, dashboard, ‡¶è‡¶¨‡¶Ç ‡¶Æ‡ßá‡¶ü‡ßç‡¶∞‡¶ø‡¶ï‡¶∏ ‡¶π‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡ßá‡¶≤ ‡¶ï‡¶∞‡ßá‡•§

3. **Ceph OSD (Object Storage Daemon)**
   ‚Üí ‡¶Æ‡ßÇ‡¶≤ ‡¶°‡ßá‡¶ü‡¶æ ‡¶∞‡¶æ‡¶ñ‡ßá ‡¶è‡¶¨‡¶Ç replication, recovery, backfilling ‡¶ï‡¶∞‡ßá‡•§

4. **Ceph MDS (Metadata Server)**
   ‚Üí File system (CephFS) ‡¶è‡¶∞ metadata manage ‡¶ï‡¶∞‡ßá‡•§

5. **RADOS (Reliable Autonomic Distributed Object Store)**
   ‚Üí Ceph-‡¶è‡¶∞ ‡¶Æ‡ßÇ‡¶≤ distributed object storage layer‡•§

---

### üì¶ **Ceph Storage ‡¶ü‡¶æ‡¶á‡¶™‡¶∏**

1. **Object Storage (RADOS Gateway - RGW)**
   ‚Üí Amazon S3/Swift compatible API ‡¶¶‡¶ø‡ßü‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá‡•§

2. **Block Storage (RBD - RADOS Block Device)**
   ‚Üí Virtual machines ‡¶¨‡¶æ bare-metal server ‡¶è use ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü (e.g., OpenStack Cinder backend)‡•§

3. **File System Storage (CephFS)**
   ‚Üí POSIX-compliant distributed file system‡•§

---

### üß± **Ceph Cluster ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ß‡¶æ‡¶™‡¶∏‡¶Æ‡ßÇ‡¶π (Step-by-Step)**

#### üß∞ ‡¶ß‡¶æ‡¶™ ‡ßß: ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶Æ ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡ßÅ‡¶§‡¶ø

* Ubuntu/RHEL server ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡ßÅ‡¶§ ‡¶ï‡¶∞‡ßÅ‡¶® (minimum 3 nodes)
* `ntp` / `chronyd` sync ‡¶ï‡¶∞‡ßÅ‡¶®
* Hostname ‡¶ì `/etc/hosts` ‡¶∏‡¶†‡¶ø‡¶ï‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶®‡¶´‡¶ø‡¶ó‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
* ‡¶´‡¶æ‡ßü‡¶æ‡¶∞‡¶ì‡ßü‡¶æ‡¶≤ ‡¶ì SELinux disable ‡¶ï‡¶∞‡ßÅ‡¶®

### üß∞ ‡¶ß‡¶æ‡¶™ ‡ß®: Ceph ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤‡ßá‡¶∂‡¶®

#### üì¶ Manual Installation:

```bash
# Ubuntu ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
sudo apt update
sudo apt install ceph-deploy ceph-common
```
#### üì¶ Or use Cephadm (for production)
#### üì¶ Or use Ceph-Ansible (for production)

### üß∞ ‡¶ß‡¶æ‡¶™ ‡ß©: Cluster Bootstrapping (ceph-deploy ‡¶¶‡¶ø‡ßü‡ßá)

```bash
ceph-deploy new mon1
ceph-deploy install mon1 osd1 osd2 mgr1
ceph-deploy mon create-initial
ceph-deploy admin mon1 osd1 osd2
```

### üß∞ ‡¶ß‡¶æ‡¶™ ‡ß™: OSD ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®

```bash
ceph-deploy osd create --data /dev/sdb osd1
ceph-deploy osd create --data /dev/sdb osd2
```

### üß∞ ‡¶ß‡¶æ‡¶™ ‡ß´: Cluster Status ‡¶Ø‡¶æ‡¶ö‡¶æ‡¶á

```bash
ceph -s
```

---

### üìä **Ceph Monitoring & Management**

* **Ceph Dashboard**: Web UI
* **Prometheus + Grafana** integration
* `ceph status`, `ceph df`, `ceph osd tree` CLI commands

---

### ‚öôÔ∏è **Ceph Storage Pools & CRUSH Map**

* **Storage Pool**: Data logical group
* **CRUSH Map**: Intelligent data placement algorithm ‡¶Ø‡¶æ redundancy ‡¶ì performance ‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§ ‡¶ï‡¶∞‡ßá

---

### üîê **Ceph ‡¶è‡¶∞ Data Redundancy & Recovery**

* **Replication**: Same data multiple OSD-‡¶§‡ßá ‡¶•‡¶æ‡¶ï‡ßá
* **Erasure Coding**: More efficient redundancy technique
* **Self-healing**: ‡¶è‡¶ï ‡¶¨‡¶æ ‡¶è‡¶ï‡¶æ‡¶ß‡¶ø‡¶ï ‡¶°‡ßá‡¶ü‡¶æ ‡¶≤‡¶∏ ‡¶π‡¶≤‡ßá‡¶ì ‡¶®‡¶ø‡¶ú‡ßá ‡¶•‡ßá‡¶ï‡ßá‡¶á restore ‡¶ï‡¶∞‡ßá

---

### üéØ **Ceph ‡¶è‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞**

| Use Case           | Explanation                |
| ------------------ | -------------------------- |
| OpenStack Backend  | Cinder, Glance, Nova       |
| Kubernetes         | Rook ‡¶¶‡¶ø‡ßü‡ßá Ceph ‡¶ö‡¶æ‡¶≤‡¶æ‡¶®‡ßã      |
| Enterprise Backup  | Large-scale object storage |
| Video Surveillance | Large file archival        |

---

---

## ‚úÖ ‡¶™‡¶∞‡ßç‡¶¨ ‡ß®: Ceph Cluster ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ (Installation + Configuration + Initial Setup)

‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶¶‡ßá‡¶ñ‡¶¨‡ßã:

* ‡ß©‡¶ü‡¶ø VM/Server ‡¶¶‡¶ø‡ßü‡ßá Cluster ‡¶¨‡¶æ‡¶®‡¶æ‡¶®‡ßã
* Ceph-deploy ‡¶¶‡¶ø‡ßü‡ßá Ceph ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶ï‡¶∞‡¶æ
* OSD, Monitor, MGR ‡¶ï‡¶®‡¶´‡¶ø‡¶ó‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ

---

### üñ•Ô∏è **Step 1: ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡ßÅ‡¶§‡¶ø (3 Node Lab Setup)**

#### üñ•Ô∏è ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ß‡¶∞‡¶õ‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡ß©‡¶ü‡¶ø Ubuntu Server ‡¶Ü‡¶õ‡ßá:

| Node Name | IP Address    | Role             |
| --------- | ------------- | ---------------- |
| mon1      | 192.168.56.10 | Monitor, Manager |
| osd1      | 192.168.56.11 | OSD Node         |
| osd2      | 192.168.56.12 | OSD Node         |

---

### üõ†Ô∏è **Step 2: ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶æ‡¶∞‡ßá ‡¶è‡¶á Common Setup ‡¶ï‡¶∞‡ßÅ‡¶®**

```bash
# Hostname ‡¶∏‡ßá‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶® (‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶®‡ßã‡¶°‡ßá ‡¶≠‡¶ø‡¶®‡ßç‡¶®‡¶≠‡¶æ‡¶¨‡ßá)
sudo hostnamectl set-hostname mon1   # ‡¶Ö‡¶•‡¶¨‡¶æ osd1, osd2

# hosts ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®
sudo nano /etc/hosts
```

```bash
192.168.56.10 mon1
192.168.56.11 osd1
192.168.56.12 osd2
```

```bash
# Update & Install basic tools
sudo apt update && sudo apt install -y ntp sshpass python3-pip
```

---

### üì• **Step 3: Ceph-deploy ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶® (Only on mon1)**

```bash
sudo apt install ceph-deploy -y
```

---

### üîê **Step 4: SSH Key ‡¶§‡ßà‡¶∞‡¶ø ‡¶è‡¶¨‡¶Ç password-less login enable ‡¶ï‡¶∞‡ßÅ‡¶®**

```bash
ssh-keygen
ssh-copy-id user@osd1
ssh-copy-id user@osd2
```

---

### üìÇ **Step 5: ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞ ‡¶á‡¶®‡¶ø‡¶∂‡¶ø‡ßü‡¶æ‡¶≤‡¶æ‡¶á‡¶ú ‡¶ï‡¶∞‡ßÅ‡¶® (mon1-‡¶è)**

```bash
mkdir ceph-cluster
cd ceph-cluster

# ‡¶®‡¶§‡ßÅ‡¶® cluster ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®
ceph-deploy new mon1
```

---

### üì¶ **Step 6: ‡¶∏‡¶Æ‡¶∏‡ßç‡¶§ Node-‡¶è Ceph ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®**

```bash
ceph-deploy install mon1 osd1 osd2
```

---

### üß† **Step 7: Monitor ‡¶è‡¶¨‡¶Ç Manager ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®**

```bash
ceph-deploy mon create-initial
ceph-deploy mgr create mon1
```

---

### üõ°Ô∏è **Step 8: Admin Key ‡¶¨‡¶ø‡¶§‡¶∞‡¶£ ‡¶ï‡¶∞‡ßÅ‡¶®**

```bash
ceph-deploy admin mon1 osd1 osd2
chmod +r /etc/ceph/ceph.client.admin.keyring
```

---

### üíæ **Step 9: OSD ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶® (‡¶ß‡¶∞‡¶ø `/dev/sdb` device ‡¶Ü‡¶õ‡ßá)**

```bash
ceph-deploy osd create --data /dev/sdb osd1
ceph-deploy osd create --data /dev/sdb osd2
```

---

### üîç **Step 10: ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞ ‡¶∏‡ßç‡¶ü‡ßá‡¶ü‡¶æ‡¶∏ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®**

```bash
ceph -s
```

‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ Output:

```bash
cluster:
  id:     3a1b-4e32-91a1-1e2443e9c9d6
  health: HEALTH_OK

services:
  mon: 1 daemons, quorum mon1
  mgr: mon1(active)
  osd: 2 osds: 2 up, 2 in

data:
  pools:   1 pools, 100 pgs
  objects: 0 objects, 0B
```

---

## üéâ ‡¶è‡¶ñ‡¶® ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ö‡¶≤‡¶Æ‡¶æ‡¶® Ceph Cluster ‡¶§‡ßà‡¶∞‡¶ø ‡¶π‡ßü‡ßá‡¶õ‡ßá!

---

# üß™ **PART 1: Single Node Ceph Cluster Setup (Lab/Test Purpose Only)**

> ‚ö†Ô∏è ‡¶è‡¶ü‡¶ø ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ Learning/Test ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø‡•§ Production-‡¶è ‡¶è‡¶ï‡¶æ‡¶ß‡¶ø‡¶ï node ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶¨‡¶æ‡¶ß‡ßç‡¶Ø‡¶§‡¶æ‡¶Æ‡ßÇ‡¶≤‡¶ï‡•§

---

### üñ•Ô∏è System Requirements:

* OS: Ubuntu 20.04+
* RAM: 4 GB+
* Disk: ‡¶è‡¶ï‡¶æ‡¶ß‡¶ø‡¶ï ‡¶°‡¶ø‡¶≠‡¶æ‡¶á‡¶∏ (‡¶è‡¶ï‡¶ü‡¶ø OS ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø, ‡¶Ö‡¶®‡ßç‡¶Ø‡¶ü‡¶ø OSD-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø, e.g. `/dev/sdb`)
* Hostname: `ceph-node`

---

### üîß Step-by-Step:

#### üîπ Step 1: Hostname & `/etc/hosts` ‡¶†‡¶ø‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®

```bash
sudo hostnamectl set-hostname ceph-node
sudo nano /etc/hosts
```

```txt
127.0.0.1 ceph-node
```

---

#### üîπ Step 2: Ceph Tools ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®

```bash
sudo apt update
sudo apt install -y ceph-deploy ceph-common
```

---

#### üîπ Step 3: Ceph Cluster ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø

```bash
mkdir ceph-cluster && cd ceph-cluster
```

---

#### üîπ Step 4: New Cluster ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®

```bash
ceph-deploy new ceph-node
```

---

#### üîπ Step 5: Ceph ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®

```bash
ceph-deploy install ceph-node
```

---

#### üîπ Step 6: Monitor ‡¶è‡¶¨‡¶Ç Manager ‡¶§‡ßà‡¶∞‡¶ø

```bash
ceph-deploy mon create-initial
ceph-deploy mgr create ceph-node
```

---

#### üîπ Step 7: Admin Key ‡¶ï‡¶™‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®

```bash
ceph-deploy admin ceph-node
```

---

#### üîπ Step 8: OSD ‡¶§‡ßà‡¶∞‡¶ø (‡¶ß‡¶∞‡¶ø `/dev/sdb` ‡¶Ü‡¶õ‡ßá)

```bash
ceph-deploy osd create --data /dev/sdb ceph-node
```

---

#### üîπ Step 9: ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®

```bash
ceph -s
```

---

‚úÖ **Single Node Ceph Ready!**

---

# üß™ **PART 2: Two Node Ceph Cluster Setup**

> üìå Example:

* Node1: `mon1` (Monitor + MGR)
* Node2: `osd1` (OSD only)

---

### üîπ Step 1: Hosts Setup

**On both:**

```bash
# Set hostname
hostnamectl set-hostname mon1      # on node1
hostnamectl set-hostname osd1      # on node2

# Update /etc/hosts on both
192.168.56.10 mon1
192.168.56.11 osd1
```

---

### üîπ Step 2: Install Ceph-deploy on `mon1`

```bash
sudo apt install ceph-deploy -y
```

---

### üîπ Step 3: SSH Key Setup from `mon1`

```bash
ssh-keygen
ssh-copy-id user@osd1
```

---

### üîπ Step 4: Cluster ‡¶§‡ßà‡¶∞‡¶ø

```bash
mkdir ceph-cluster && cd ceph-cluster
ceph-deploy new mon1
```

---

### üîπ Step 5: Install Ceph on all

```bash
ceph-deploy install mon1 osd1
```

---

### üîπ Step 6: Create MON & MGR

```bash
ceph-deploy mon create-initial
ceph-deploy mgr create mon1
```

---

### üîπ Step 7: Admin key push

```bash
ceph-deploy admin mon1 osd1
```

---

### üîπ Step 8: Create OSD on osd1

```bash
ceph-deploy osd create --data /dev/sdb osd1
```

---

### üîπ Step 9: Status Check

```bash
ceph -s
```

‚úÖ Two node Ceph cluster ‡¶§‡ßà‡¶∞‡¶ø ‚úÖ

---

# üß™ **PART 3: Three Node Production-Grade Ceph Cluster Setup**

| Hostname | Role      | IP            |
| -------- | --------- | ------------- |
| mon1     | MON + MGR | 192.168.56.10 |
| osd1     | OSD       | 192.168.56.11 |
| osd2     | OSD       | 192.168.56.12 |

Same steps as above ‚Äî ‡¶∂‡ßÅ‡¶ß‡ßÅ `ceph-deploy install mon1 osd1 osd2` ‡¶è‡¶¨‡¶Ç `osd create` ‡¶§‡¶ø‡¶®‡¶ü‡¶ø‡¶§‡ßá ‡¶ï‡¶∞‡¶¨‡ßá‡¶®‡•§

---

# üê≥ **PART 4: Docker ‡¶¶‡¶ø‡ßü‡ßá Ceph Cluster ‡¶ö‡¶æ‡¶≤‡¶æ‡¶®‡ßã (Lab/Test Purpose Only)**

> ‚ö†Ô∏è Docker-‡¶è Ceph production ‡¶è use ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡¶®‡¶æ‡•§ ‡¶è‡¶ü‡¶ø ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ educational/testing ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø‡•§

---

### üß± Docker Compose ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá Ceph ‡¶ö‡¶æ‡¶≤‡¶æ‡¶§‡ßá ‡¶ö‡¶æ‡¶á‡¶≤‡ßá: (via [Ceph-Docker](https://github.com/ceph/ceph-docker))

#### üîπ Step 1: Clone the repo

```bash
git clone https://github.com/ceph/ceph-docker.git
cd ceph-docker
```

#### üîπ Step 2: Prepare your config

Use sample `docker-compose.yml`:

```yaml
version: '3'
services:
  ceph-mon:
    image: ceph/daemon
    environment:
      - MON_IP=192.168.56.10
      - CEPH_PUBLIC_NETWORK=192.168.56.0/24
      - MON_NAME=mon
    volumes:
      - /etc/ceph:/etc/ceph
      - /var/lib/ceph/:/var/lib/ceph/
    network_mode: host
```

#### üîπ Step 3: Run Docker Compose

```bash
docker-compose up -d
```

#### üîπ Step 4: Exec inside container

```bash
docker exec -it ceph-mon bash
ceph -s
```

---

‡¶è‡¶ñ‡¶æ‡¶®‡ßá **Docker Compose ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá Ceph ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤‡ßá‡¶∂‡¶® ‡¶ì ‡¶ï‡¶®‡¶´‡¶ø‡¶ó‡¶æ‡¶∞‡ßá‡¶∂‡¶®** ‡¶è‡¶∞ ‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶ó‡¶æ‡¶á‡¶° ‡¶ß‡¶æ‡¶™‡ßá ‡¶ß‡¶æ‡¶™‡ßá ‚Äî ‡¶™‡ßç‡¶∞‡¶æ‡¶•‡¶Æ‡¶ø‡¶ï ‡¶•‡ßá‡¶ï‡ßá ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶°‡¶≠‡¶æ‡¶®‡ßç‡¶∏‡¶° ‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§‡•§

---

# üöÄ PART 1: Docker Compose ‡¶¶‡¶ø‡ßü‡ßá Ceph Storage Setup (Step-by-Step)

> ‚úÖ ‡¶Ü‡¶™‡¶®‡¶ø ‡¶∏‡¶π‡¶ú‡ßá‡¶á Block Storage (RBD), Object Storage (RGW), ‡¶è‡¶¨‡¶Ç File System (CephFS) ‡¶ö‡¶æ‡¶≤‡¶æ‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßá‡¶®‡•§

---

## üß± Step 1: System Requirements

* OS: Ubuntu 20.04 / 22.04 (or any Linux distro with Docker)
* RAM: 4‚Äì8 GB+ (more for full services)
* Docker: version 20.x+
* Docker Compose: version 1.25+

---

## ‚öôÔ∏è Step 2: Docker & Docker Compose ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®

```bash
sudo apt update
sudo apt install -y docker.io docker-compose
sudo systemctl enable --now docker
```

> üîë Optional: Add your user to Docker group:

```bash
sudo usermod -aG docker $USER
newgrp docker
```

---

## üì¶ Step 3: Create Project Folder

```bash
mkdir ceph-docker && cd ceph-docker
```

---

## üìù Step 4: Docker Compose ‡¶´‡¶æ‡¶á‡¶≤ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®

```bash
nano docker-compose.yml
```

‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶è‡¶ï‡¶ü‡¶ø Minimal Ceph Cluster Compose config:

```yaml
version: '3'

services:
  ceph-mon:
    image: ceph/daemon
    network_mode: "host"
    environment:
      - MON_IP=127.0.0.1
      - CEPH_PUBLIC_NETWORK=127.0.0.1/24
      - MON_NAME=mon
      - CLUSTER=ceph
      - DEBUG=verbose
      - RGW_CIVETWEB_PORT=8080
    volumes:
      - /etc/ceph:/etc/ceph
      - /var/lib/ceph:/var/lib/ceph
    restart: always
```

> ‚ÑπÔ∏è ‡¶è‡¶á Compose ‡¶´‡¶æ‡¶á‡¶≤‡¶ü‡¶ø Ceph Monitor (MON), Manager (MGR), ‡¶è‡¶¨‡¶Ç Object Gateway (RGW) ‡¶∏‡¶π ‡¶è‡¶ï‡¶ü‡¶ø Single Node cluster ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶¨‡ßá‡•§

---

## ‚ñ∂Ô∏è Step 5: Run the Ceph Container

```bash
docker-compose up -d
```

Check logs:

```bash
docker logs -f ceph-docker_ceph-mon_1
```

---

## üîç Step 6: Container-‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶ó‡¶ø‡ßü‡ßá Ceph Status ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®

```bash
docker exec -it ceph-docker_ceph-mon_1 bash
ceph -s
```

> ‚úÖ ‡¶Ø‡¶¶‡¶ø ‡¶∏‡¶¨ ‡¶†‡¶ø‡¶ï ‡¶•‡¶æ‡¶ï‡ßá, ‡¶Ü‡¶™‡¶®‡¶ø ‡¶¶‡ßá‡¶ñ‡¶¨‡ßá‡¶®: `HEALTH_OK`, MON running, OSD ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá‡¶ì ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§

---

# üß† PART 2: Add Advanced Components

---

## üß± Add OSD (Object Storage Device)

* Ceph OSD ‡¶ö‡¶æ‡¶≤‡¶æ‡¶§‡ßá ‡¶π‡¶≤‡ßá ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ volume ‡¶¨‡¶æ block device ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞‡•§

### üîß Example OSD container:

```yaml
  ceph-osd:
    image: ceph/daemon
    network_mode: "host"
    environment:
      - OSD_TYPE=directory
      - OSD_DIRECTORY=/var/lib/ceph/osd/osd1
      - CLUSTER=ceph
    volumes:
      - /etc/ceph:/etc/ceph
      - /var/lib/ceph:/var/lib/ceph
    restart: always
```

> üìÅ ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ö‡¶æ‡¶á‡¶≤‡ßá `/data/osd1` ‡¶®‡¶æ‡¶Æ‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶°‡¶ø‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®‡•§

```bash
mkdir -p /var/lib/ceph/osd/osd1
```

---

## ‚òÅÔ∏è Add RGW (Object Storage Gateway like S3)

RGW ‡¶ö‡¶æ‡¶≤‡¶æ‡¶§‡ßá ‡¶π‡¶≤‡ßá `RGW_CIVETWEB_PORT` ‡¶∏‡¶π Config ‡¶¶‡¶ø‡¶®:

```yaml
  ceph-rgw:
    image: ceph/daemon
    network_mode: "host"
    environment:
      - RGW_NAME=rgw1
      - RGW_CIVETWEB_PORT=8080
      - CLUSTER=ceph
    volumes:
      - /etc/ceph:/etc/ceph
      - /var/lib/ceph:/var/lib/ceph
    restart: always
```

üì¶ Access RGW via:

```
http://localhost:8080/
```

---

## üìÅ Add CephFS (File System Support)

CephFS ‡¶ö‡¶æ‡¶≤‡¶æ‡¶§‡ßá `ceph-mds` container ‡¶ö‡¶æ‡¶≤‡¶æ‡¶§‡ßá ‡¶π‡¶¨‡ßá:

```yaml
  ceph-mds:
    image: ceph/daemon
    network_mode: "host"
    environment:
      - CLUSTER=ceph
      - MDS_NAME=mds1
    volumes:
      - /etc/ceph:/etc/ceph
      - /var/lib/ceph:/var/lib/ceph
    restart: always
```

---

## üìä Add Ceph Dashboard

```yaml
  ceph-mgr:
    image: ceph/daemon
    network_mode: "host"
    environment:
      - CLUSTER=ceph
      - MGR_NAME=mgr
      - ENABLE_CEPH_DASHBOARD=true
    volumes:
      - /etc/ceph:/etc/ceph
      - /var/lib/ceph:/var/lib/ceph
    restart: always
```

Dashboard Access:

```
https://localhost:8443/
```

Login Setup:

```bash
ceph dashboard set-login-credentials admin yourpassword
```

---

# üìò PART 3: Ceph ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‚Äì RBD, CephFS, RGW

## üì¶ RBD (Block Device):

```bash
ceph osd pool create rbd 128
rbd create mydisk --size 10240
rbd map mydisk
mkfs.ext4 /dev/rbd0
mount /dev/rbd0 /mnt
```

---

## ‚òÅÔ∏è RGW (S3 Compatible)

### Create S3 User:

```bash
radosgw-admin user create --uid="testuser" --display-name="Test User"
```

### Get S3 credentials:

```bash
radosgw-admin user info --uid="testuser"
```

---

## üìÅ CephFS:

```bash
ceph fs volume create myfs
mount -t ceph 127.0.0.1:6789:/ /mnt -o name=admin,secretfile=/etc/ceph/ceph.client.admin.keyring
```

---

# üõ°Ô∏è Security and Tips

| Practice                | Description                  |
| ----------------------- | ---------------------------- |
| Use separate volumes    | Avoid data loss              |
| Enable SSL for RGW      | Use HTTPS                    |
| Monitor with Prometheus | Metrics scraping             |
| Backup config files     | `/etc/ceph`, `/var/lib/ceph` |

---

# ‚úÖ Summary

| Step | What You Did                    |
| ---- | ------------------------------- |
| 1    | Docker & Compose Setup          |
| 2    | Minimal Ceph Cluster ‡¶§‡ßà‡¶∞‡¶ø       |
| 3    | Ceph MON/MGR/RGW ‡¶ö‡¶æ‡¶≤‡ßÅ           |
| 4    | Advanced Components Add         |
| 5    | RBD, RGW, CephFS ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶∂‡¶ø‡¶ñ‡¶≤‡ßá‡¶® |

---




# âœ… Kubernetes + Ceph Full Production Blueprint

(Enterprise Design + Real Use Case + Step-by-Step)

---

# ğŸ“Œ 1ï¸âƒ£ Production Goal

à¦†à¦®à¦¾à¦¦à§‡à¦° à¦²à¦•à§à¦·à§à¦¯:

- Kubernetes cluster (HA control plane)
    
- Ceph cluster (RBD + CephFS + RGW)
    
- Dynamic provisioning
    
- High availability
    
- Monitoring + Backup
    
- Zero single point of failure
    

---

# ğŸ“Œ 2ï¸âƒ£ High-Level Architecture Diagram

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚         End Users            â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                        Load Balancer
                               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                      â”‚                      â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Master1   â”‚        â”‚  Master2   â”‚        â”‚  Master3   â”‚
  â”‚ (Control)  â”‚        â”‚ (Control)  â”‚        â”‚ (Control)  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              Worker Nodes                   â”‚
        â”‚  App Pods + CSI Driver + RBD/CephFS Mount  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚          Ceph Cluster          â”‚
                â”‚ MON + MGR + OSD + MDS + RGW   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ“Œ 3ï¸âƒ£ Production Environment Example

## ğŸ¢ Scenario: SaaS Company

Workload:

- 50 microservices
    
- PostgreSQL database
    
- File upload service
    
- Daily backup
    
- Log archival
    

Storage mapping:

|Service|Ceph Type|
|---|---|
|PostgreSQL|RBD|
|Shared media|CephFS|
|Backup|RGW|

---

# ğŸ“Œ 4ï¸âƒ£ Step 1: Ceph Cluster Ready à¦¥à¦¾à¦•à¦¾

Prerequisite:

- 3 MON
    
- 3+ OSD node
    
- RBD pool created
    
- CephFS created
    
- RGW running
    

(à¦†à¦®à¦°à¦¾ à¦†à¦—à§‡à¦° à¦…à¦§à§à¦¯à¦¾à§Ÿà§‡ à¦à¦—à§à¦²à§‹ à¦•à¦°à§‡à¦›à¦¿)

---

# ğŸ“Œ 5ï¸âƒ£ Step 2: Ceph CSI Driver Deploy à¦•à¦°à¦¾

Kubernetes à¦à¦–à¦¨ CSI driver à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡à¥¤

Deploy Ceph CSI:

```bash
kubectl create namespace ceph-csi
```

Secret à¦¤à§ˆà¦°à¦¿ à¦•à¦°à§à¦¨:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
  namespace: ceph-csi
stringData:
  userID: kubernetes
  userKey: <ceph-key>
```

Apply:

```bash
kubectl apply -f secret.yaml
```

---

# ğŸ“Œ 6ï¸âƒ£ Step 3: RBD StorageClass à¦¤à§ˆà¦°à¦¿ à¦•à¦°à¦¾

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-rbd
provisioner: rbd.csi.ceph.com
parameters:
  clusterID: <cluster-id>
  pool: rbd_pool
  imageFeatures: layering
reclaimPolicy: Delete
allowVolumeExpansion: true
```

Apply:

```bash
kubectl apply -f rbd-sc.yaml
```

---

# ğŸ“Œ 7ï¸âƒ£ Step 4: RBD PVC Example (Database)

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
spec:
  storageClassName: ceph-rbd
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
```

Apply:

```bash
kubectl apply -f pvc.yaml
```

Pod à¦ attach à¦•à¦°à¦²à§‡ dynamic RBD image à¦¤à§ˆà¦°à¦¿ à¦¹à¦¬à§‡à¥¤

---

# ğŸ“Œ 8ï¸âƒ£ Real Scenario: PostgreSQL Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
spec:
  template:
    spec:
      containers:
      - name: postgres
        image: postgres:15
        volumeMounts:
        - mountPath: "/var/lib/postgresql/data"
          name: db-storage
      volumes:
      - name: db-storage
        persistentVolumeClaim:
          claimName: postgres-pvc
```

Result:

- PostgreSQL data Ceph RBD à¦¤à§‡ à¦¥à¦¾à¦•à¦¬à§‡
    
- Pod restart à¦¹à¦²à§‡à¦“ data safe
    

---

# ğŸ“Œ 9ï¸âƒ£ CephFS StorageClass (Shared Storage)

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: cephfs-sc
provisioner: cephfs.csi.ceph.com
parameters:
  fsName: mycephfs
  pool: cephfs_data
```

PVC:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-media
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: cephfs-sc
  resources:
    requests:
      storage: 50Gi
```

---

# ğŸ“Œ ğŸ”Ÿ Real Scenario: Web App with Shared Upload

- 3 replica web pod
    
- Same upload directory
    

All pods mount CephFS PVCà¥¤

Result:

- Any pod upload â†’ visible to all
    
- No NFS bottleneck
    
- Highly scalable
    

---

# ğŸ“Œ 1ï¸âƒ£1ï¸âƒ£ Backup Using RGW + Velero

Velero config:

```yaml
spec:
  provider: aws
  objectStorage:
    bucket: k8s-backup
```

Ceph RGW acts as S3 backendà¥¤

Result:

- Namespace backup
    
- PVC snapshot backup
    
- Disaster recovery ready
    

---

# ğŸ“Œ 1ï¸âƒ£2ï¸âƒ£ Scaling Example

More traffic â†’ scale app:

```bash
kubectl scale deployment web --replicas=10
```

Ceph automatically:

- Handle more IOPS
    
- Distribute load across OSD
    

No storage downtimeà¥¤

---

# ğŸ“Œ 1ï¸âƒ£3ï¸âƒ£ High Availability Model

Production Setup:

- 3 Master node
    
- 5+ Worker node
    
- 3 MON
    
- 6+ OSD
    
- Separate network
    

Failure Example:

Worker node crash â†’ Pod reschedule â†’ PVC reattach â†’ No data loss

---

# ğŸ“Œ 1ï¸âƒ£4ï¸âƒ£ Performance Tuning

For Database workload:

- Use SSD-backed pool
    
- Enable RBD exclusive-lock
    
- Use separate pool for DB
    

For File workload:

- Increase MDS count
    

---

# ğŸ“Œ 1ï¸âƒ£5ï¸âƒ£ Monitoring Integration

Monitor:

- Ceph Dashboard
    
- Prometheus
    
- Kubernetes metrics
    
- Alertmanager
    

Alert example:

- PVC nearly full
    
- OSD high latency
    
- Node disk pressure
    

---

# ğŸ“Œ 1ï¸âƒ£6ï¸âƒ£ Security Best Practice

- Separate Ceph user for Kubernetes
    
- Limited pool permission
    
- Network isolation
    
- TLS enabled CSI
    

---

# ğŸ“Œ 1ï¸âƒ£7ï¸âƒ£ Production Capacity Example

Suppose:

- 6 OSD
    
- 10TB each
    
- Replication 3
    

Raw: 60TB  
Usable: 20TB

Plan 70% threshold maxà¥¤

---

# ğŸ“Œ 1ï¸âƒ£8ï¸âƒ£ Full Data Flow Summary

```
User â†’ LoadBalancer â†’ Pod
Pod â†’ PVC â†’ CSI
CSI â†’ Ceph (RBD/CephFS)
Ceph â†’ OSD â†’ Disk
```

---

# ğŸ¯ Final Production Blueprint Summary

âœ” HA Kubernetes  
âœ” Dynamic Storage Provisioning  
âœ” RBD for Database  
âœ” CephFS for Shared Storage  
âœ” RGW for Backup  
âœ” Monitoring + Alert  
âœ” Scaling without downtime  
âœ” DR Ready

---

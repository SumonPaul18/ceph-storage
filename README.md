# ğŸš€ Learn Ceph - The Ultimate Guide to Ceph Storage System

Welcome to **Learn Ceph**, a fully open-source, beginner-to-advanced guide to mastering the **Ceph Distributed Storage System**. Whether you're a system administrator, DevOps engineer, or a storage enthusiast â€” this repository will help you understand, install, configure, monitor, and scale Ceph like a pro. ğŸš€

![Ceph-Storage](./src/images/ceph-wide.png)

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![GitHub Contributors](https://img.shields.io/github/contributors/SumonPaul18/ceph-storage)](https://github.com/SumonPaul18/ceph-storage/graphs/contributors)
[![GitHub Issues](https://img.shields.io/github/issues/SumonPaul18/ceph-storage)](https://github.com/SumonPaul18/ceph-storage/issues)
[![GitHub Pull Requests](https://img.shields.io/badge/pulls-0-blue.svg)](https://github.com/SumonPaul18/ceph-storage/pulls)

---

## ğŸ“š Table of Contents

- [ğŸ“– What is Ceph?](#-what-is-ceph)
- [âœ¨ Features](#-features)
- [ğŸ§± Ceph Architecture](#-ceph-architecture)
- [âš™ï¸ Installation](#-installation-methods)
- [ğŸ› ï¸ Configuration](#%EF%B8%8F-configuration--management)
- [ğŸ’» Common Ceph Commands](#-common-ceph-commands)
- [ğŸ“Š Monitoring](#-monitoring)
- [ğŸ“¦ Use Cases](#-use-cases)
- [ğŸ“˜ Tutorials](#-tutorials)
- [ğŸ“Œ FAQs](#-faqs)
- [ğŸ¤ Contributing](#-contributing)


---

## ğŸ“– What is Ceph?

**Ceph** is a powerful, scalable, and reliable distributed storage system designed to provide:
- **Block storage** (RBD)
- **Object storage** (RGW/S3 compatible)
- **File system** (CephFS)

It is fault-tolerant, self-healing, self-managing, and ideal for modern cloud-native infrastructure.

---

## âœ¨ Features

| Feature                  | Description                                                                 |
|--------------------------|-----------------------------------------------------------------------------|
| ğŸ”„ Scalability            | Grows from GBs to multi-PB seamlessly                                      |
| ğŸ”§ Unified Storage        | Block, Object, and File storage in one platform                            |
| ğŸ›  Self-healing            | Automatically recovers from hardware or disk failures                      |
| ğŸ§  Intelligent Clustering | Uses CRUSH algorithm for data placement without bottlenecks                |
| ğŸ” High Availability      | No single point of failure, supports replication and erasure coding        |
| ğŸ’» Open Source            | Backed by a large community and enterprise-ready (Red Hat Ceph, etc.)      |

---

## ğŸ§± Ceph Architecture

Cephâ€™s architecture consists of the following components:

| Component | Description |
|----------|-------------|
| **MON (Monitor)** | Maintains cluster map and health |
| **MGR (Manager)** | Provides dashboard and monitoring |
| **OSD (Object Storage Daemon)** | Stores actual data on disk |
| **MDS (Metadata Server)** | Manages metadata for CephFS |
| **Client Interfaces** | RBD, RGW (S3), CephFS for applications |

ğŸ“– [Detailed Architecture Guide â†’](./docs/architecture/README.md)

---

## ğŸš€ Installation Methods

You can deploy Ceph in multiple ways. We provide detailed, step-by-step guides for each:

1. ğŸ”§ [Deploy using Cephadm (Recommended)](./docs/installation/cephadm/cephadm.md) â€“ Production-ready and simple
2. ğŸ§° [Install on Bare Metal (Manual Setup)](./docs/installation/bare-metal.md) â€“ Full control
3. ğŸ³ [Install Ceph using Docker Compose](./docs/installation/docker-compose.md) â€“ Ideal for testing

ğŸ“Œ Minimum Requirements:
- Linux OS (Ubuntu/CentOS)
- 2+ CPUs, 4GB+ RAM per node
- SSD/HDD for OSDs
- Proper network configuration

---

## âš™ï¸ Configuration & Management

Ceph is highly configurable. This section covers essential configurations:

- [ğŸ“„ Understanding `ceph.conf`](./docs/configuration/README.md)
- [ğŸ— Pools & Replication](./docs/configuration/pools.md)
- [ğŸ§  CRUSH Map and Data Placement](./docs/configuration/crush-map.md)

> Each topic includes examples, diagrams, and best practices.

---

## ğŸ’» Common Ceph Commands

- [Ceph CLI Basics](./docs/commands/README.md)
- [Troubleshooting Tips](./docs/commands/troubleshooting.md)

Examples:

```
ceph status
```
```
ceph osd tree
```
```
ceph df
```
```
ceph health detail
```

---

## ğŸ“Š Monitoring

Monitoring is essential for understanding the performance and health of your Ceph cluster. Ceph provides several built-in and third-party tools for visualization and alerting.

### ğŸ”­ Tools & Integrations

| Tool         | Description                                  |
|--------------|----------------------------------------------|
| Ceph Dashboard | Built-in web-based UI for monitoring cluster |
| Prometheus    | Metrics collection and alerting              |
| Grafana       | Visualize Ceph metrics from Prometheus       |
| Alertmanager  | Define and route alerts                      |

### ğŸ›  How to Set Up

```bash
# Enable Prometheus module
ceph mgr module enable prometheus

# Export metrics endpoint (default: 9283)
curl http://<ceph-mgr>:9283/metrics
````

ğŸ“˜ For detailed setup: [`/monitoring-and-alerting/overview.md`](./docs/monitoring-and-alerting/README.md)

---

## ğŸ“¦ Use Cases

Ceph is highly flexible and supports multiple real-world use cases across various industries:

### ğŸ–¼ Object Storage (Ceph RGW)

* S3 & Swift-compatible API
* Ideal for backups, logs, media assets

### ğŸ§  Block Storage (Ceph RBD)

* Persistent volumes for VMs & containers
* Integration with OpenStack & Kubernetes

### ğŸ“ File Storage (CephFS)

* POSIX-compliant distributed file system
* Suitable for HPC, AI/ML workloads

### ğŸ¢ Enterprise Scenarios

* Hybrid-cloud deployments
* Scalable archival & video surveillance storage

> ğŸ’¡ **Did you know?** Leading platforms like OpenStack, Kubernetes, and SUSE use Ceph as their backend storage solution.

---

## ğŸ“˜ Tutorials

Want hands-on experience? Check out these guides:
* ğŸ—ï¸ [Ceph Architecture Overview](./docs/architecture/)
* âš™ï¸ [Ceph Installation & Deployment Guide](./docs/installation/)
* ğŸ–¥ï¸ [Ceph Configuration in Depth](./docs/configuration/)
* ğŸ¤– [Ceph (CLI) & Troubleshooting Reference](./docs/commands/)
* ğŸ“¡ [Ceph Monitoring & Alerting Guide](./docs/monitoring-and-alerting/)

ğŸ“ More coming soon: CephFS snapshots, Multi-site RGW, Ceph on Kubernetes, etc.

---

## ğŸ“Œ FAQs

### â“ What is the minimum number of nodes for a production Ceph cluster?

At least **3 MONs** and **3 OSDs** are recommended for quorum and redundancy.

---

### â“ Is Ceph suitable for small-scale or home lab setups?

âœ… Yes. With tools like cephadm and containers, even a single-node deployment is possible.

---

### â“ How does Ceph handle node failure?

Ceph uses data replication (or erasure coding) and automatic rebalancing to maintain availability even after hardware failure.

---

### â“ Can I use Ceph with Kubernetes?

Absolutely. Ceph integrates well via **Rook**, **CSI drivers**, and **RBD/NFS** backends for dynamic provisioning.

---

## ğŸ¤ Contributing

We welcome contributions from the community! ğŸš€

### ğŸ›  How to Contribute

1. Fork this repository
2. Create a new branch: `git checkout -b feature/your-feature`
3. Make your changes
4. Submit a Pull Request

### ğŸ“ƒ Guidelines

* Follow markdown consistency and formatting
* Add relevant diagrams if helpful
* Keep explanations simple and clear
* Reference official documentation when possible

### ğŸ™Œ Contributors

Thanks to all the contributors who make this project better every day. ğŸ’™

---

> Made with â¤ï¸ by [Suman Pal](https://github.com/SumonPaul18) | Ceph â¤ï¸ Open Source

---

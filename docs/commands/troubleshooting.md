# ğŸ› ï¸ Troubleshooting Ceph

Running a Ceph cluster in production can occasionally lead to issues like OSD down, degraded PGs, or MON quorum loss. Here's how to identify and resolve them.

---

## ğŸ§­ Table of Contents

- [ğŸš¨ Common Issues](#-common-issues)
- [ğŸ“‹ Logs & Health](#-logs--health)
- [ğŸ§ª OSD Recovery](#-osd-recovery)
- [ğŸ” PG Stuck / Degraded](#-pg-stuck--degraded)
- [ğŸ’¡ MON Quorum Issues](#-mon-quorum-issues)
- [ğŸ›¡ Slow Ops & Performance](#-slow-ops--performance)
- [ğŸ“š Resources](#-resources)

---

## ğŸš¨ Common Issues

| Problem                        | Symptom                           | Solution                                      |
|-------------------------------|------------------------------------|-----------------------------------------------|
| OSD Down                      | `osd.X down`                      | Restart OSD, check logs                       |
| MON Quorum Lost              | `no quorum` warning                | Ensure 3 MONs, check network/firewall         |
| PGs Stuck or Degraded        | `pgs degraded, stuck`             | Check `ceph pg dump`, ensure all OSDs are in  |
| Slow Ops                     | `slow ops` warning                | Review client ops, check OSD/mgr stats        |

---

## ğŸ“‹ Logs & Health

```bash
ceph status
ceph health detail
ceph log last 50
journalctl -u ceph-osd@0
journalctl -u ceph-mon@ceph-mon1
````

### ğŸ” Check specific daemon logs:

```bash
sudo journalctl -u ceph-<daemon>@<id> -f
```

---

## ğŸ§ª OSD Recovery

```bash
ceph osd tree
ceph osd out 3
systemctl restart ceph-osd@3
ceph osd in 3
```

### ğŸ§  If an OSD is consistently failing:

* Check disk health: `smartctl -a /dev/sdX`
* Replace disk, re-add OSD

---

## ğŸ” PG Stuck / Degraded

```bash
ceph pg dump | grep "stuck"
ceph pg repair <pg_id>
```

### ğŸ” To force repair:

```bash
ceph pg repair 1.23
```

### âœ… Ensure:

* All OSDs are `up` and `in`
* Network connectivity is fine
* Time is synced (use `chrony` or `ntpd`)

---

## ğŸ’¡ MON Quorum Issues

### Symptoms:

```bash
ceph status
# shows "no quorum"
```

### Solutions:

```bash
# Check MON map
ceph mon dump

# Restart MON
systemctl restart ceph-mon@ceph-mon1

# Check quorum status
ceph quorum_status --format json-pretty
```

> ğŸ§  Ensure all MONs have correct entries in `/etc/hosts` and ceph.conf

---

## ğŸ›¡ Slow Ops & Performance

### Investigate:

```bash
ceph health detail
ceph osd perf
ceph tell osd.* bench
```

### Actions:

* Identify overloaded OSDs
* Use `iostat`, `iotop`, or `dstat` for system analysis
* Check for slow disks

---

## ğŸ“š Resources

* [Official Ceph Troubleshooting Guide](https://docs.ceph.com/en/latest/rados/troubleshooting/)
* [cephadm logs](https://docs.ceph.com/en/latest/cephadm/services/)

---
